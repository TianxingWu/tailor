{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "partial-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "written-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(file_name):\n",
    "    df = pd.read_csv(file_name, sep=\"\\t\")\n",
    "    # Filter failed perturbation strategies\n",
    "    df = df[df['valid?']!='--']\n",
    "    \n",
    "    # Sample 50 unique indices\n",
    "    np.random.seed(SEED)\n",
    "    indices = np.random.choice(df['idx'].unique(), 50, replace=False)\n",
    "    df = df[df['idx'].isin(indices)]\n",
    "    yes_indices = set(df[df['valid?']=='Y']['idx'].unique())\n",
    "    maybe_indices = set(df[df['valid?']=='M']['idx'].unique())\n",
    "    num_successful = len(yes_indices.union(maybe_indices))\n",
    "    print(f\"num successful: {num_successful}\")\n",
    "    print(f\"validity: {num_successful/50}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "personal-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexisr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_valid_sample(df):\n",
    "    np.random.seed(SEED)\n",
    "    num_invalid=0\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    for didx in df['idx'].unique():\n",
    "        temp_df = df[df['idx']==didx]\n",
    "        valid_df = temp_df[temp_df['valid?'].isin([\"Y\", \"M\"])]\n",
    "        if len(valid_df) == 0:\n",
    "            num_invalid += 1\n",
    "            continue\n",
    "        row = valid_df.sample()\n",
    "        new_df = new_df.append(row)\n",
    "    print(f\"{num_invalid}/{len(df['idx'].unique())} invalid\")\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n",
    "import difflib\n",
    "import more_itertools as mit\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "PREPOSITIONS = [\"at\", \"above\", \"across\", \"after\", \"against\", \"along\", \"among\", \"around\", \"at\", \"before\",\n",
    "               \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"by\", \"down\", \"for\",\"from\", \"in\",\n",
    "               \"into\", \"near\", \"of\", \"off\", \"on\", \"to\", \"toward\", \"under\", \"upon\", \"with\", \"within\"]\n",
    "\n",
    "def get_edited_spans(orig, edited, tokenizer_wrapper=SpacyTokenizer(), return_type='new'):\n",
    "    \"\"\" Given an orig and edited inputs, mark up differences in HTML. \"\"\"\n",
    "\n",
    "    orig_tok = tokenizer_wrapper.tokenize(orig)\n",
    "    edited_tok = tokenizer_wrapper.tokenize(edited)\n",
    "\n",
    "    orig_text_tok = [t.text for t in orig_tok]\n",
    "    edited_text_tok = [t.text for t in edited_tok]\n",
    "\n",
    "    edited_mark_indices, num_add, num_del = get_marked_indices(orig_text_tok, \n",
    "                                                        edited_text_tok, \"+\")\n",
    "    orig_mark_indices, num_add_2, num_del_2 = get_marked_indices(orig_text_tok, \n",
    "                                                        edited_text_tok, \"-\")\n",
    "    if return_type == \"orig\":\n",
    "        indices = orig_mark_indices\n",
    "        tok = orig_tok\n",
    "        text = orig\n",
    "    else:\n",
    "        indices = edited_mark_indices\n",
    "        tok = edited_tok\n",
    "        text = edited\n",
    "    \n",
    "    grouped_indices = mit.consecutive_groups(indices)\n",
    "\n",
    "    spans = []\n",
    "    for group in grouped_indices:\n",
    "        temp_span = []\n",
    "\n",
    "        for tok_idx, idx in enumerate(group):\n",
    "            token = tok[idx]\n",
    "            start, end = token.idx, token.idx_end\n",
    "            if start == None or end == None:\n",
    "                logger.info(token, start, end)\n",
    "            \n",
    "            tok_text = text[start:end]\n",
    "            if tok_idx == 0 and tok_text.lower() in PREPOSITIONS:\n",
    "                continue\n",
    "                \n",
    "            if tok_text in stopwords.words('english'):\n",
    "                continue\n",
    "\n",
    "            if any([c.isalnum() for c in tok_text]) and len(tok_text) > 0:\n",
    "                temp_span.append(tok_text)\n",
    "        if temp_span == []:\n",
    "            continue\n",
    "        spans.append(\" \".join(temp_span))\n",
    "\n",
    "    return spans\n",
    "\n",
    "def get_marked_indices(orig_tokinal, tokenized_contrast, symbol):\n",
    "    \"\"\" Helper function for html_highlight_diffs. \n",
    "    Will only return indices of words deleted or replaced (not inserted). \"\"\"\n",
    "\n",
    "    index_offset = 0\n",
    "    d = difflib.Differ()\n",
    "    diff = d.compare(orig_tokinal, tokenized_contrast)\n",
    "    list_diff = list(diff)\n",
    "    tokens, modified_tokens, indices = [], [], []\n",
    "    counter = 0\n",
    "    additions, deletions = 0, 0\n",
    "\n",
    "    for token_idx, token in enumerate(list_diff):\n",
    "        marker = token[0]\n",
    "        word = token[2:]\n",
    "        if marker == symbol:        \n",
    "            tokens.append(word)\n",
    "            indices.append(counter)\n",
    "            counter += 1\n",
    "        elif marker == \" \":\n",
    "            modified_tokens.append(word)\n",
    "            counter += 1\n",
    "\n",
    "        if marker == \"+\":\n",
    "            additions += 1\n",
    "        if marker == \"-\":\n",
    "            deletions += 1\n",
    "            \n",
    "    return indices, additions, deletions\n",
    "\n",
    "def heuristically_pick_span(row, span_key):\n",
    "    spans = row[span_key]\n",
    "    if len(spans) == 1:\n",
    "        return spans[0]\n",
    "\n",
    "    temp_spans = [s for s in spans if s not in row['original']]\n",
    "    if len(spans) == 1:\n",
    "        return spans[0]\n",
    "    if len(temp_spans) != 0:\n",
    "        spans = temp_spans\n",
    "    if spans == []:\n",
    "        return \"\"\n",
    "    num_toks = [len(\" \".split(s)) for s in spans]\n",
    "    idx = np.argmax(num_toks)\n",
    "    return spans[idx]\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def measure_diversity(new_df):\n",
    "\n",
    "    new_df['human_spans'] = new_df.apply(lambda row: get_edited_spans(row['original'], row['human_perturbed']), axis=1)\n",
    "    new_df['tailor_spans'] = new_df.apply(lambda row: get_edited_spans(row['original'], row['tailor_perturbed']), axis=1)\n",
    "\n",
    "    new_df['human_span'] = new_df.apply(lambda row: heuristically_pick_span(row, 'human_spans'), axis=1)\n",
    "    new_df['tailor_span'] = new_df.apply(lambda row: heuristically_pick_span(row, 'tailor_spans'), axis=1)\n",
    "\n",
    "    new_df['human_ngrams'] = new_df.apply(lambda row: list(ngrams(nltk.word_tokenize(row['human_span']), 1)), axis=1)\n",
    "    new_df['tailor_ngrams'] = new_df.apply(lambda row: list(ngrams(nltk.word_tokenize(row['tailor_span']), 1)), axis=1)\n",
    "\n",
    "    ngrams_dict = {}\n",
    "    for key in ['tailor_ngrams', 'human_ngrams']:\n",
    "        fdist = nltk.FreqDist(new_df[key].sum())\n",
    "        print(\"\\nKEY:\", key)\n",
    "        ngrams_dict[key] = set(fdist.keys())\n",
    "        num_unique = (len(fdist.keys()))\n",
    "        print(f\"num unique: {num_unique}\")\n",
    "        num_total = len(new_df[key].sum())\n",
    "        print(f\"num total: {num_total}\")\n",
    "        print(f\"ratio: {num_unique/num_total}\")\n",
    "        \n",
    "    overlap_tokens = len(ngrams_dict['tailor_ngrams'] & ngrams_dict['human_ngrams'])\n",
    "    print(f\"num overlap tokens: {overlap_tokens}\")\n",
    "    print(f\"num tailor tokens: {len(ngrams_dict['tailor_ngrams'])}\")\n",
    "    print(f\"num human tokens: {len(ngrams_dict['human_ngrams'])}\")\n",
    "    return ngrams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "artificial-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num successful: 43\n",
      "validity: 0.86\n",
      "num successful: 20\n",
      "validity: 0.4\n"
     ]
    }
   ],
   "source": [
    "noun2verb = process_df('noun2verb.csv')\n",
    "verb2noun = process_df('verb2noun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "therapeutic-north",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/50 invalid\n",
      "\n",
      "KEY: tailor_ngrams\n",
      "num unique: 60\n",
      "num total: 77\n",
      "ratio: 0.7792207792207793\n",
      "\n",
      "KEY: human_ngrams\n",
      "num unique: 54\n",
      "num total: 62\n",
      "ratio: 0.8709677419354839\n",
      "num overlap tokens: 9\n",
      "num tailor tokens: 60\n",
      "num human tokens: 54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tailor_ngrams': {(\"'m\",),\n",
       "  ('Canada',),\n",
       "  ('Comic',),\n",
       "  ('Corps',),\n",
       "  ('Dvo√°k',),\n",
       "  ('Have',),\n",
       "  ('I',),\n",
       "  ('March',),\n",
       "  ('Monday',),\n",
       "  ('act',),\n",
       "  ('bringing',),\n",
       "  ('cases',),\n",
       "  ('collects',),\n",
       "  ('comprehensive',),\n",
       "  ('course',),\n",
       "  ('day',),\n",
       "  ('different',),\n",
       "  ('e-',),\n",
       "  ('fair',),\n",
       "  ('family',),\n",
       "  ('first',),\n",
       "  ('free',),\n",
       "  ('get',),\n",
       "  ('grounds',),\n",
       "  ('justification',),\n",
       "  ('kind',),\n",
       "  ('know',),\n",
       "  ('language',),\n",
       "  ('last',),\n",
       "  ('least',),\n",
       "  ('life',),\n",
       "  ('link',),\n",
       "  ('living',),\n",
       "  ('long',),\n",
       "  ('make',),\n",
       "  ('many',),\n",
       "  ('minute',),\n",
       "  ('moral',),\n",
       "  ('one',),\n",
       "  ('order',),\n",
       "  ('orders',),\n",
       "  ('papers',),\n",
       "  ('past',),\n",
       "  ('reasonable',),\n",
       "  ('respect',),\n",
       "  ('rest',),\n",
       "  ('series',),\n",
       "  ('sessions',),\n",
       "  ('spite',),\n",
       "  ('technique',),\n",
       "  ('terrorist',),\n",
       "  ('three',),\n",
       "  ('time',),\n",
       "  ('two',),\n",
       "  ('version',),\n",
       "  ('view',),\n",
       "  ('way',),\n",
       "  ('whole',),\n",
       "  ('year',),\n",
       "  ('years',)},\n",
       " 'human_ngrams': {('2002',),\n",
       "  ('November',),\n",
       "  ('basis',),\n",
       "  ('beginning',),\n",
       "  ('benefit',),\n",
       "  ('caution',),\n",
       "  ('change',),\n",
       "  ('charge',),\n",
       "  ('circumstances',),\n",
       "  ('convincing',),\n",
       "  ('country',),\n",
       "  ('daily',),\n",
       "  ('dawn',),\n",
       "  ('day',),\n",
       "  ('decade',),\n",
       "  ('earliest',),\n",
       "  ('email',),\n",
       "  ('end',),\n",
       "  ('evening',),\n",
       "  ('extra',),\n",
       "  ('first',),\n",
       "  ('great',),\n",
       "  ('grounds',),\n",
       "  ('honesty',),\n",
       "  ('hours',),\n",
       "  ('initiative',),\n",
       "  ('knife',),\n",
       "  ('last',),\n",
       "  ('light',),\n",
       "  ('list',),\n",
       "  ('manner',),\n",
       "  ('many',),\n",
       "  ('matter',),\n",
       "  ('minutes',),\n",
       "  ('moment',),\n",
       "  ('month',),\n",
       "  ('morning',),\n",
       "  ('next',),\n",
       "  ('opportunity',),\n",
       "  ('practical',),\n",
       "  ('price',),\n",
       "  ('quarter',),\n",
       "  ('reasonable',),\n",
       "  ('reasons',),\n",
       "  ('response',),\n",
       "  ('safety',),\n",
       "  ('significant',),\n",
       "  ('table',),\n",
       "  ('terms',),\n",
       "  ('time',),\n",
       "  ('unexpected',),\n",
       "  ('way',),\n",
       "  ('ways',),\n",
       "  ('years',)}}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = get_valid_sample(noun2verb)\n",
    "measure_diversity(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "contained-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num successful: 20\n",
      "validity: 0.4\n",
      "30/50 invalid\n",
      "\n",
      "KEY: tailor_ngrams\n",
      "num unique: 29\n",
      "num total: 29\n",
      "ratio: 1.0\n",
      "\n",
      "KEY: human_ngrams\n",
      "num unique: 29\n",
      "num total: 29\n",
      "ratio: 1.0\n",
      "num overlap tokens: 2\n",
      "num tailor tokens: 29\n",
      "num human tokens: 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tailor_ngrams': {('.',),\n",
       "  (\"It's'gimmick\",),\n",
       "  ('New',),\n",
       "  ('President',),\n",
       "  ('Russian',),\n",
       "  ('U.S',),\n",
       "  ('York',),\n",
       "  ('accident',),\n",
       "  ('around',),\n",
       "  ('audience',),\n",
       "  ('bacon',),\n",
       "  ('corner',),\n",
       "  ('economy',),\n",
       "  ('father',),\n",
       "  ('guys',),\n",
       "  ('inside',),\n",
       "  ('issues',),\n",
       "  ('matter',),\n",
       "  ('mission',),\n",
       "  ('needing',),\n",
       "  ('ocean',),\n",
       "  ('pigeons',),\n",
       "  ('profits',),\n",
       "  ('run',),\n",
       "  ('side',),\n",
       "  ('speeding',),\n",
       "  ('studying',),\n",
       "  ('swans',),\n",
       "  ('wide',)},\n",
       " 'human_ngrams': {('2',),\n",
       "  ('Europe',),\n",
       "  ('Russian',),\n",
       "  ('adjusted',),\n",
       "  ('beef',),\n",
       "  ('beginning',),\n",
       "  ('bracket',),\n",
       "  ('city',),\n",
       "  ('complicated',),\n",
       "  ('dollars',),\n",
       "  ('government',),\n",
       "  ('highest',),\n",
       "  ('inflation',),\n",
       "  ('inside',),\n",
       "  ('optimization',),\n",
       "  ('origin',),\n",
       "  ('popular',),\n",
       "  ('project',),\n",
       "  ('robbery',),\n",
       "  ('roundabout',),\n",
       "  ('saying',),\n",
       "  ('scholar',),\n",
       "  ('sign',),\n",
       "  ('subject',),\n",
       "  ('tables',),\n",
       "  ('tax',),\n",
       "  ('team',),\n",
       "  ('teams',),\n",
       "  ('tower',)}}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb2noun = process_df('verb2noun.csv')\n",
    "new_df = get_valid_sample(verb2noun)\n",
    "measure_diversity(new_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
